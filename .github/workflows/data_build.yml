# =============================================================================
# DATA BUILD PIPELINE - CI/CD WORKFLOW
# =============================================================================
# Automated builds with quality gates for the Tatnall Legacy data pipeline.
#
# Triggers:
# - Push to main/develop branches
# - Scheduled (weekly)
# - Manual dispatch
#
# Features:
# - Validation gates (must pass audit)
# - Deploy only on success
# - Notifications on failure
# =============================================================================

name: Data Build Pipeline

on:
  # Trigger on push to main branches
  push:
    branches:
      - main
      - develop
    paths:
      - 'scripts/**'
      - 'build.config.yaml'
      - 'data/**'
      - '.github/workflows/data_build.yml'

  # Scheduled builds (every Sunday at 2 AM UTC)
  schedule:
    - cron: '0 2 * * 0'

  # Manual trigger with options
  workflow_dispatch:
    inputs:
      force_rebuild:
        description: 'Force full rebuild (ignore cache)'
        required: false
        default: 'false'
        type: boolean
      seasons:
        description: 'Seasons to build (comma-separated, e.g., 2024,2025)'
        required: false
        default: ''
        type: string
      skip_deploy:
        description: 'Skip deployment step'
        required: false
        default: 'false'
        type: boolean

# Prevent concurrent builds
concurrency:
  group: data-build-${{ github.ref }}
  cancel-in-progress: true

env:
  PYTHON_VERSION: '3.11'
  NODE_VERSION: '18'

jobs:
  # ===========================================================================
  # Validation Job
  # ===========================================================================
  validate:
    name: Validate Inputs
    runs-on: ubuntu-latest

    outputs:
      should_build: ${{ steps.check.outputs.should_build }}

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Check if build is needed
        id: check
        run: |
          # Always build on schedule or manual trigger
          if [ "${{ github.event_name }}" = "schedule" ] || [ "${{ github.event_name }}" = "workflow_dispatch" ]; then
            echo "should_build=true" >> $GITHUB_OUTPUT
            exit 0
          fi

          # Check if relevant files changed
          CHANGED_FILES=$(git diff --name-only ${{ github.event.before }} ${{ github.sha }} || echo "")

          if echo "$CHANGED_FILES" | grep -qE '^(scripts/|build\.config\.yaml|data/)'; then
            echo "should_build=true" >> $GITHUB_OUTPUT
          else
            echo "should_build=false" >> $GITHUB_OUTPUT
          fi

  # ===========================================================================
  # Build Job
  # ===========================================================================
  build:
    name: Build Data Pipeline
    runs-on: ubuntu-latest
    needs: validate
    if: needs.validate.outputs.should_build == 'true'

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 0  # Full history for change detection

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install pandas numpy scipy pyyaml

      - name: Restore build cache
        uses: actions/cache@v4
        with:
          path: |
            .build_state
            data_raw
          key: build-cache-${{ runner.os }}-${{ hashFiles('build.config.yaml') }}
          restore-keys: |
            build-cache-${{ runner.os }}-

      - name: Verify inputs
        run: |
          echo "Verifying input data availability..."
          python scripts/verify_inputs.py || true

      - name: Run data pipeline
        id: build
        run: |
          ARGS=""

          if [ "${{ github.event.inputs.force_rebuild }}" = "true" ]; then
            ARGS="$ARGS --force"
          fi

          if [ -n "${{ github.event.inputs.seasons }}" ]; then
            ARGS="$ARGS --seasons ${{ github.event.inputs.seasons }}"
          fi

          echo "Running pipeline with args: $ARGS"
          python scripts/pipeline/orchestrator.py $ARGS --parallel --workers 2

      - name: Run validation
        id: validate
        run: |
          echo "Running data validation..."
          python scripts/validation/validate.py --ci --report-file validation_report.json

      - name: Run audit
        id: audit
        run: |
          echo "Running data audit..."
          python scripts/audit/full_audit.py --ci --fail-threshold 80 --output audit_report.json

      - name: Upload validation report
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: validation-report
          path: validation_report.json
          retention-days: 30

      - name: Upload audit report
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: audit-report
          path: audit_report.json
          retention-days: 30

      - name: Save build state
        uses: actions/cache/save@v4
        if: success()
        with:
          path: |
            .build_state
            data_raw
          key: build-cache-${{ runner.os }}-${{ hashFiles('build.config.yaml') }}-${{ github.sha }}

  # ===========================================================================
  # Export Job
  # ===========================================================================
  export:
    name: Export Site Data
    runs-on: ubuntu-latest
    needs: build
    if: success()

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install pandas numpy scipy pyyaml

      - name: Restore build cache
        uses: actions/cache@v4
        with:
          path: |
            .build_state
            data_raw
          key: build-cache-${{ runner.os }}-${{ hashFiles('build.config.yaml') }}-${{ github.sha }}

      - name: Export site data
        run: |
          echo "Exporting optimized site data..."
          python scripts/export/build_site_data.py --all --minify

      - name: Build search index
        run: |
          echo "Building search index..."
          python scripts/export/build_search_index.py --lightweight --minify

      - name: Build API cache
        run: |
          echo "Building API cache..."
          python scripts/export/build_api_cache.py --all --page-size 25

      - name: Upload site data artifact
        uses: actions/upload-artifact@v4
        with:
          name: site-data
          path: public/data
          retention-days: 7

      - name: Upload API cache artifact
        uses: actions/upload-artifact@v4
        with:
          name: api-cache
          path: public/api
          retention-days: 7

  # ===========================================================================
  # Deploy Job
  # ===========================================================================
  deploy:
    name: Deploy to Production
    runs-on: ubuntu-latest
    needs: export
    if: |
      success() &&
      github.ref == 'refs/heads/main' &&
      github.event.inputs.skip_deploy != 'true'

    environment:
      name: production
      url: https://your-site.com/data

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Download site data
        uses: actions/download-artifact@v4
        with:
          name: site-data
          path: public/data

      - name: Download API cache
        uses: actions/download-artifact@v4
        with:
          name: api-cache
          path: public/api

      # Example: Deploy to GitHub Pages
      - name: Deploy to GitHub Pages
        if: false  # Disabled by default - enable and configure for your deployment
        uses: peaceiris/actions-gh-pages@v3
        with:
          github_token: ${{ secrets.GITHUB_TOKEN }}
          publish_dir: ./public
          destination_dir: data

      # Example: Deploy to S3
      - name: Deploy to S3
        if: false  # Disabled by default - enable and configure for your deployment
        uses: jakejarvis/s3-sync-action@master
        with:
          args: --follow-symlinks --delete
        env:
          AWS_S3_BUCKET: ${{ secrets.AWS_S3_BUCKET }}
          AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          SOURCE_DIR: 'public'

      # Example: Deploy via SSH/rsync
      - name: Deploy via SSH
        if: false  # Disabled by default - enable and configure for your deployment
        uses: burnett01/rsync-deployments@5.2
        with:
          switches: -avzr --delete
          path: public/
          remote_path: /var/www/data/
          remote_host: ${{ secrets.DEPLOY_HOST }}
          remote_user: ${{ secrets.DEPLOY_USER }}
          remote_key: ${{ secrets.DEPLOY_KEY }}

      - name: Deployment complete
        run: |
          echo "Deployment complete!"
          echo "Site data deployed to production."

  # ===========================================================================
  # Notify Job
  # ===========================================================================
  notify:
    name: Send Notifications
    runs-on: ubuntu-latest
    needs: [build, export, deploy]
    if: always()

    steps:
      - name: Determine build status
        id: status
        run: |
          if [ "${{ needs.build.result }}" = "failure" ]; then
            echo "status=failure" >> $GITHUB_OUTPUT
            echo "message=Data build failed" >> $GITHUB_OUTPUT
          elif [ "${{ needs.export.result }}" = "failure" ]; then
            echo "status=failure" >> $GITHUB_OUTPUT
            echo "message=Data export failed" >> $GITHUB_OUTPUT
          elif [ "${{ needs.deploy.result }}" = "failure" ]; then
            echo "status=failure" >> $GITHUB_OUTPUT
            echo "message=Deployment failed" >> $GITHUB_OUTPUT
          else
            echo "status=success" >> $GITHUB_OUTPUT
            echo "message=Data build and deploy successful" >> $GITHUB_OUTPUT
          fi

      # Example: Slack notification
      - name: Send Slack notification
        if: false  # Disabled by default - enable and configure
        uses: 8398a7/action-slack@v3
        with:
          status: ${{ steps.status.outputs.status }}
          text: ${{ steps.status.outputs.message }}
          fields: repo,message,commit,author,action,eventName,ref,workflow
        env:
          SLACK_WEBHOOK_URL: ${{ secrets.SLACK_WEBHOOK_URL }}

      # Example: Discord notification
      - name: Send Discord notification
        if: false  # Disabled by default - enable and configure
        uses: sarisia/actions-status-discord@v1
        with:
          webhook: ${{ secrets.DISCORD_WEBHOOK }}
          status: ${{ steps.status.outputs.status }}
          title: "Data Build Pipeline"
          description: ${{ steps.status.outputs.message }}

      - name: Log completion
        run: |
          echo "Build workflow completed"
          echo "Status: ${{ steps.status.outputs.status }}"
          echo "Message: ${{ steps.status.outputs.message }}"

  # ===========================================================================
  # Cleanup Job
  # ===========================================================================
  cleanup:
    name: Cleanup Old Artifacts
    runs-on: ubuntu-latest
    needs: [deploy]
    if: success() && github.ref == 'refs/heads/main'

    steps:
      - name: Delete old artifacts
        uses: actions/github-script@v7
        with:
          script: |
            const { owner, repo } = context.repo;

            // Get all artifacts
            const artifacts = await github.rest.actions.listArtifactsForRepo({
              owner,
              repo,
              per_page: 100,
            });

            // Delete artifacts older than 7 days
            const cutoff = new Date();
            cutoff.setDate(cutoff.getDate() - 7);

            for (const artifact of artifacts.data.artifacts) {
              const createdAt = new Date(artifact.created_at);
              if (createdAt < cutoff) {
                console.log(`Deleting artifact: ${artifact.name} (${artifact.id})`);
                await github.rest.actions.deleteArtifact({
                  owner,
                  repo,
                  artifact_id: artifact.id,
                });
              }
            }
